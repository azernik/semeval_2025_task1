{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azernik/semeval_2025_task1/blob/main/tuning_open_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### setup"
      ],
      "metadata": {
        "id": "dCwMhir9e3ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for downloading results from Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown"
      ],
      "metadata": {
        "id": "1BzSx1mtsu8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_IG3pcVvXr9"
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udfv44jGJC1x"
      },
      "outputs": [],
      "source": [
        "# download taskA file from Adam's Drive (public) and unzip\n",
        "file_id = \"105JdQU_u98w_xSYaNNSj-r4RsyTPXZEF\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(url, \"taskA.zip\", quiet=True)\n",
        "! unzip -q - taskA.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEpy5AG8vexN"
      },
      "outputs": [],
      "source": [
        "import open_clip\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from ast import literal_eval\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import re\n",
        "from itertools import combinations\n",
        "\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxmBGIhbJe1I"
      },
      "outputs": [],
      "source": [
        "# define locations\n",
        "taska_folder = \"train\"\n",
        "taska_tsv_filename = \"subtask_a_train.tsv\"\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv(f\"{taska_folder}/{taska_tsv_filename}\", delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC5Pn-j1v1p2"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ5BFNR4KYLC"
      },
      "outputs": [],
      "source": [
        "# Preprocess dataframe (image paths, etc.)\n",
        "image_name_cols = ['image1_name', 'image2_name', 'image3_name', 'image4_name', 'image5_name']\n",
        "df['image_paths'] = df.apply(lambda row: [os.path.join(taska_folder, row['compound'].replace(\"'\", \"_\"), row[image_name]) for image_name in image_name_cols], axis=1)\n",
        "df['image_idx_map'] = df.apply(lambda row: {row[name]: i for i, name in enumerate(image_name_cols)}, axis=1)\n",
        "df['expected_order_indices'] = df.apply(lambda row: [row['image_idx_map'][name] for name in literal_eval(row['expected_order'])], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC-RWElvv7bL"
      },
      "outputs": [],
      "source": [
        "sentences = df.sentence\n",
        "compounds = df.compound.apply(lambda x: x.replace(\"'\", \"_\"))\n",
        "targets = [literal_eval(t) for t in df.expected_order]\n",
        "s_types = df.sentence_type\n",
        "image_paths = df['image_paths']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluation methods"
      ],
      "metadata": {
        "id": "CGNSR8_0eocY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRN5vxPMJtvE"
      },
      "outputs": [],
      "source": [
        "def evaluate_predictions(predictions, df, weights=[0.4, 0.3, 0.2, 0.1, 0.0]):\n",
        "    \"\"\"\n",
        "    Takes predictions, returns three types of evaluation metrics:\n",
        "    - Top-1 Accuracy\n",
        "    - Average Spearman Correlation\n",
        "    - Average Weighted Accuracy\n",
        "    \"\"\"\n",
        "    correct_top1 = 0\n",
        "    spearman_scores, weighted_scores = [], []\n",
        "\n",
        "    for i in range(len(predictions)):\n",
        "        # if len(predictions[i]) == 0:\n",
        "        #     continue\n",
        "\n",
        "        # Ground truth and predictions\n",
        "        # pred_order = [df['image_idx_map'].iloc[i][os.path.basename(df['image_paths'].iloc[i][j])] for j in predictions[i]]\n",
        "        pred_order = predictions[i]\n",
        "        ground_truth_order = df['expected_order_indices'].iloc[i]\n",
        "\n",
        "        # Top-1 accuracy\n",
        "        if pred_order[0] == ground_truth_order[0]:\n",
        "            correct_top1 += 1\n",
        "\n",
        "        # Spearman correlation\n",
        "        score, _ = spearmanr(pred_order, ground_truth_order)\n",
        "        spearman_scores.append(score)\n",
        "\n",
        "        # Weighted accuracy\n",
        "        weighted_score = sum(weights[j] for j, img in enumerate(pred_order) if img == ground_truth_order[j])\n",
        "        weighted_scores.append(weighted_score)\n",
        "\n",
        "    return {\n",
        "        \"top1_accuracy\": correct_top1 / len(predictions),\n",
        "        \"average_spearman\": sum(spearman_scores) / len(spearman_scores),\n",
        "        \"average_weighted_accuracy\": sum(weighted_scores) / len(weighted_scores),\n",
        "        \"spearman_scores\": spearman_scores,\n",
        "        \"weighted_scores\": weighted_scores\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzjdIlTfs7r5"
      },
      "outputs": [],
      "source": [
        "def save_results(experiment_name, base_model, model_name, metrics, results_file=\"experiment_results.csv\"):\n",
        "    \"\"\"\n",
        "    Save experiment results to a CSV file.\n",
        "    \"\"\"\n",
        "    # Add experiment name to metrics\n",
        "    results_row = {\n",
        "        \"base_model\": base_model,\n",
        "        \"model\": model_name,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"top1_accuracy\": metrics[\"top1_accuracy\"],\n",
        "        \"average_spearman\": metrics[\"average_spearman\"],\n",
        "        \"average_weighted_accuracy\": metrics[\"average_weighted_accuracy\"],\n",
        "    }\n",
        "\n",
        "    # Write results to CSV\n",
        "    write_header = not os.path.exists(results_file)\n",
        "    with open(results_file, mode=\"a\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=results_row.keys())\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(results_row)\n",
        "\n",
        "    print(f\"Results saved to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq8YWQHEtIfp"
      },
      "outputs": [],
      "source": [
        "def save_predictions(df, image_paths, predictions, confidence_scores, metrics, prefix, preds_dir='predictions'):\n",
        "    \"\"\"\n",
        "    Save detailed predictions and confidence scores for each example.\n",
        "    \"\"\"\n",
        "    # create 'preds' directory if doesn't exist\n",
        "    if not os.path.exists(preds_dir):\n",
        "        os.makedirs(preds_dir)\n",
        "\n",
        "    # generate output filename\n",
        "    prefix = prefix.strip().replace(\" \", \"_\")\n",
        "    prefix = re.sub(r'[^a-zA-Z0-9_-]', '', prefix)\n",
        "    output_path = f\"{preds_dir}/{prefix}_preds.csv\"\n",
        "\n",
        "    spearman_scores = metrics[\"spearman_scores\"]\n",
        "    weighted_scores = metrics[\"weighted_scores\"]\n",
        "    with open(output_path, mode=\"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"index\", \"compound\", \"ground_truth_order\", \"predicted_order\", \"top1_score\", \"spearman_score\", \"weighted_score\", \"confidence_scores\"])\n",
        "\n",
        "        for i, (pred, conf) in enumerate(zip(predictions, confidence_scores)):\n",
        "            # pred_order = [df['image_idx_map'].iloc[i][os.path.basename(image_paths.iloc[i][j])] for j in pred]\n",
        "            pred_order = pred\n",
        "            ground_truth_order = df[\"expected_order_indices\"].iloc[i]\n",
        "            top1_score = 1 if pred_order[0] == ground_truth_order[0] else 0\n",
        "            spearman_score = round(spearman_scores[i], 3)\n",
        "            weighted_score = round(weighted_scores[i], 3)\n",
        "            formatted_conf_scores = [round(c.item(), 3) for c in conf]\n",
        "            writer.writerow([i, df[\"compound\"].iloc[i], ground_truth_order, pred_order, top1_score, spearman_score, weighted_score, formatted_conf_scores])\n",
        "\n",
        "    print(f\"Predictions saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBV5Udz_vj2g"
      },
      "outputs": [],
      "source": [
        "def openclip_image_ranking(model, image_processor, tokenizer, image_paths, sentence):\n",
        "    image_inputs = torch.stack([image_processor(Image.open(ipath)) for ipath in image_paths]).to(device)\n",
        "    text_input = tokenizer([sentence]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_inputs)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # normalise features\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # dot product & softmax\n",
        "    similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
        "\n",
        "    # order by similarity\n",
        "    probs, indices = similarity[0].topk(5)\n",
        "    return probs, indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training and dataset functions"
      ],
      "metadata": {
        "id": "yQWrH8QZeuyq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCNBUqVIYvry"
      },
      "outputs": [],
      "source": [
        "def openclip_train(model, tokenizer, image_preprocess, dataloader, optimizer):\n",
        "    # one epoch only\n",
        "    # image paths are ordered by how similar they should be to the sentence\n",
        "    model.train()\n",
        "    margin = 0.1\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        texts, imgs1, imgs2 = batch\n",
        "\n",
        "        # encode text and images - always the preferred image is img1\n",
        "        text_input = tokenizer(texts).to(device)\n",
        "\n",
        "        text_features = model.encode_text(text_input, normalize=True)\n",
        "\n",
        "        image_inputs1 = torch.stack([image_preprocess(Image.open(ipath)) for ipath in imgs1]).to(device)\n",
        "        image_features1 = model.encode_image(image_inputs1, normalize=True)\n",
        "\n",
        "        image_inputs2 = torch.stack([image_preprocess(Image.open(ipath)) for ipath in imgs2]).to(device)\n",
        "        image_features2 = model.encode_image(image_inputs2, normalize=True)\n",
        "\n",
        "        # dot product\n",
        "        B, D = text_features.shape\n",
        "        similarities1 = torch.bmm(text_features.view(B, 1, D), image_features1.view(B, D, 1)) # expected to be more similar\n",
        "        similarities1 = similarities1.squeeze(-1)\n",
        "        similarities2 = torch.bmm(text_features.view(B, 1, D), image_features2.view(B, D, 1)) # expected to be less similar\n",
        "        similarities2 = similarities2.squeeze(-1)\n",
        "\n",
        "        # compare logits\n",
        "        contrastive_loss = torch.nn.functional.relu(margin + similarities2 - similarities1).sum() # less - more to give -ve diff and 0 loss if correct\n",
        "\n",
        "        # update params\n",
        "        contrastive_loss.backward()\n",
        "        # print(contrastive_loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "def openclip_evaluate(model, tokenizer, image_preprocess, test_sentences, test_image_paths, test_targets, verbose=True):\n",
        "    model.eval()\n",
        "    predictions, confidence = [], []\n",
        "    for s, ipaths, tgt in zip(test_sentences, test_image_paths, test_targets):\n",
        "        sorted_probs, ids_sorted = openclip_image_ranking(model, image_preprocess, tokenizer, ipaths, s)\n",
        "        predictions.append(ids_sorted.tolist())\n",
        "        confidence.append(100 * sorted_probs)\n",
        "    return predictions, confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RypeYIaCprCj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Custom Dataset\n",
        "class PairwiseDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, img1, img2 = self.data[idx]\n",
        "        # Add logic to load image tensors if needed\n",
        "        return text, img1, img2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIYUiA-hpHp1"
      },
      "outputs": [],
      "source": [
        "def reorder_image_paths(ipaths, tgt):\n",
        "    tgt_order = {t:j for j, t in enumerate(tgt)}\n",
        "    ordered_ipaths = sorted(ipaths, key=lambda x: tgt_order[x.split('/')[-1]])\n",
        "    return ordered_ipaths\n",
        "\n",
        "# image_paths are the original order\n",
        "# ordered_image_paths are in the target order, which is useful for training\n",
        "# (but also means if you evaluate performance on the training set using the standard function it looks terrible)\n",
        "\n",
        "image_paths_copy = image_paths.copy()\n",
        "ordered_image_paths = [reorder_image_paths(ipaths, targets[i]) for i, ipaths in enumerate(image_paths_copy)]\n",
        "\n",
        "def split_train_and_test_data(test_indices, train_indices, input_text):\n",
        "    testing_sentences = [input_text[idx] for idx in test_indices]\n",
        "    training_sentences = [input_text[idx] for idx in train_indices]\n",
        "\n",
        "    testing_targets = [targets[idx] for idx in test_indices]\n",
        "    training_targets = [targets[idx] for idx in train_indices]\n",
        "\n",
        "    testing_image_paths = [image_paths[idx] for idx in test_indices]\n",
        "    training_image_paths = [ordered_image_paths[idx] for idx in train_indices]\n",
        "\n",
        "    return {'train': (training_sentences, training_image_paths, training_targets),\n",
        "            'test': (testing_sentences, testing_image_paths, testing_targets)}\n",
        "\n",
        "def make_pairwise(sentences, image_paths):\n",
        "    pairwise_dataset = []\n",
        "    for text, images in zip(sentences, image_paths):\n",
        "        pairs = list(combinations(range(len(images)), 2))  # All 10 pairs\n",
        "        for i, j in pairs:\n",
        "            pairwise_dataset.append((text, images[i], images[j]))\n",
        "    return pairwise_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get data and run training loop"
      ],
      "metadata": {
        "id": "v5ibqJl9ezzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file of prompt responses from Google Drive\n",
        "gdown.download(\"https://drive.google.com/uc?id=1T9pMSMj6JQP0DCLy-6H7dfUtVRWy39uq\", 'gpt_prompt_responses.csv', quiet=False)"
      ],
      "metadata": {
        "id": "UT2xvxl2sFZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-Ybw-57yugQ"
      },
      "outputs": [],
      "source": [
        "df_text_inputs = pd.read_csv('gpt_prompt_responses.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains two epochs across 10 splits of the data to get results for each sample.\n",
        "The results are collected together then evaluated and saved."
      ],
      "metadata": {
        "id": "A9xAzSoOtR0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffTVFDBGsSIG"
      },
      "outputs": [],
      "source": [
        "num_groups = 10\n",
        "order_for_testing = torch.randperm(len(sentences))\n",
        "testing_groups = torch.chunk(order_for_testing, num_groups)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "all_predictions = {i:\n",
        "    {'preds': [[0,1,2,3,4]]*len(sentences), 'conf': [[0.2,0.2,0.2,0.2,0.2]]*len(sentences)}\n",
        "    for i in range(num_epochs)}\n",
        "\n",
        "experiment_name = 'baseline_sentences'\n",
        "base_model = 'openclip'\n",
        "model_name = 'ViT-B-32_finetune'\n",
        "\n",
        "for i in range(len(testing_groups)):\n",
        "    test_indices = testing_groups[i].tolist()\n",
        "    train_indices = torch.concat(testing_groups[:i] + testing_groups[i+1:]).tolist()\n",
        "    split_data = split_train_and_test_data(test_indices, train_indices, sentences)\n",
        "\n",
        "    pairwise_train_data = make_pairwise(*split_data['train'][:2])\n",
        "\n",
        "    # Initialize Dataset and DataLoader for training\n",
        "    pairwise_Dataset = PairwiseDataset(pairwise_train_data)\n",
        "    pairwise_dataloader = DataLoader(pairwise_Dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    # prep model\n",
        "    model_openclip, _, preprocess_openclip = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "    model_openclip.to(device)\n",
        "    # model_openclip.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
        "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    model_openclip.train()\n",
        "\n",
        "    # don't train the image part of the model\n",
        "    for param in model_openclip.visual.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # # only train the image part of the model\n",
        "    # for name, param in model_openclip.named_parameters():\n",
        "    #     if not name.startswith('visual'):\n",
        "    #         param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_openclip.parameters()), lr=1e-6)\n",
        "\n",
        "    # pre training eval\n",
        "    model = model_openclip\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'training split {i}, epoch {epoch}')\n",
        "        model = openclip_train(model, tokenizer, preprocess_openclip, pairwise_dataloader, optimizer)\n",
        "        predictions, confidence = openclip_evaluate(model, tokenizer, preprocess_openclip, *split_data['test'], verbose=False)\n",
        "\n",
        "        for j, orig_id in enumerate(test_indices):\n",
        "            all_predictions[epoch]['preds'][orig_id] = predictions[j]\n",
        "            all_predictions[epoch]['conf'][orig_id] = confidence[j]\n",
        "\n",
        "# print(all_predictions)\n",
        "for epoch in range(num_epochs):\n",
        "    preds = all_predictions[epoch]['preds']\n",
        "    conf = all_predictions[epoch]['conf']\n",
        "    results = evaluate_predictions(preds, df)\n",
        "\n",
        "    save_results(experiment_name, base_model, model_name+f'_e{epoch}', results, results_file=\"experiment_results.csv\")\n",
        "\n",
        "    prefix = experiment_name+'_'+base_model+'_'+model_name+f'_e{epoch}'\n",
        "    save_predictions(df, None, preds, conf, results, prefix, preds_dir='predictions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP-elPrvi4ZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykx-qiAI3b4y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "dCwMhir9e3ML",
        "CGNSR8_0eocY",
        "yQWrH8QZeuyq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}