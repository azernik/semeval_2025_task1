{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aErtAe0NV-9S",
        "yPbvQvgBUFsO",
        "uZgJsl5kTP6X",
        "xWGGZrLU4UqP",
        "HHui7nG3OjuZ",
        "i6n4c9bBe_6-",
        "6s2KxV54fG5j"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46e263745bfe42cba1641ccd22a583df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_009c03518f504c37929deeb8fdc1f45f",
              "IPY_MODEL_6f63427ed5524f2b853db233a9930ba1",
              "IPY_MODEL_c9996590005b4a6e8452ab677d8b8bdd"
            ],
            "layout": "IPY_MODEL_e8d60c1dc5984c828b91609211e2542e"
          }
        },
        "009c03518f504c37929deeb8fdc1f45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_789460fd175a46dfbaeec46cada512b7",
            "placeholder": "​",
            "style": "IPY_MODEL_a16b29c632334c38afaa2f0c5300d17a",
            "value": "open_clip_pytorch_model.bin: 100%"
          }
        },
        "6f63427ed5524f2b853db233a9930ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f90ee054b17415795c3c85d1984c96d",
            "max": 605219813,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fc90f4f752141dfb14c4d4e02c85c1a",
            "value": 605219813
          }
        },
        "c9996590005b4a6e8452ab677d8b8bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_322054a5c1c547d197c1213ebed9ffea",
            "placeholder": "​",
            "style": "IPY_MODEL_56537b9780ab48489916fc4ad79ff42f",
            "value": " 605M/605M [00:02&lt;00:00, 231MB/s]"
          }
        },
        "e8d60c1dc5984c828b91609211e2542e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "789460fd175a46dfbaeec46cada512b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16b29c632334c38afaa2f0c5300d17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f90ee054b17415795c3c85d1984c96d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc90f4f752141dfb14c4d4e02c85c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "322054a5c1c547d197c1213ebed9ffea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56537b9780ab48489916fc4ad79ff42f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azernik/semeval_2025_task1/blob/main/admire_experiments_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Preprocessing"
      ],
      "metadata": {
        "id": "22DMs3xNaE-K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMi9EZE4ZGFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c169e65-afb2-455b-fbff-77a33c1273a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# for downloading the train zip from Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# install clip\n",
        "!pip install -q ftfy regex tqdm\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import clip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anyio==3.5.0 openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ],
      "metadata": {
        "id": "6cAArqDy5BqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download taskA file from Adam's Drive (public) and unzip\n",
        "file_id = \"105JdQU_u98w_xSYaNNSj-r4RsyTPXZEF\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(url, \"taskA.zip\", quiet=True)\n",
        "! unzip -q - taskA.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aFXAYlFl8urF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495cb77e-6627-4323-be84-ea41acfca81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace train/acid test/02817176209.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# define locations\n",
        "taska_folder = \"train\"\n",
        "taska_tsv_filename = \"subtask_a_train.tsv\"\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv(f\"{taska_folder}/{taska_tsv_filename}\", delimiter=\"\\t\")\n",
        "\n",
        "# fix incorrect row in dataset\n",
        "df.loc[df['compound'] == \"pain in the neck\", 'sentence_type'] = 'literal'"
      ],
      "metadata": {
        "id": "1MtprhczrZL8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[['compound', 'sentence', 'sentence_type', 'expected_order_indices']]"
      ],
      "metadata": {
        "id": "DVyUOGFLsZ4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "# Preprocess dataframe (image paths, etc.)\n",
        "image_name_cols = ['image1_name', 'image2_name', 'image3_name', 'image4_name', 'image5_name']\n",
        "df['image_paths'] = df.apply(lambda row: [os.path.join(taska_folder, row['compound'].replace(\"'\", \"_\"), row[image_name]) for image_name in image_name_cols], axis=1)\n",
        "df['image_idx_map'] = df.apply(lambda row: {row[name]: i for i, name in enumerate(image_name_cols)}, axis=1)\n",
        "df['expected_order_indices'] = df.apply(lambda row: [row['image_idx_map'][name] for name in literal_eval(row['expected_order'])], axis=1)"
      ],
      "metadata": {
        "id": "PoELS4WoZtci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model-specific functions"
      ],
      "metadata": {
        "id": "PIKWFMqPumBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from ast import literal_eval\n",
        "\n",
        "def get_image_ranking_clip(model, image_processor, image_paths, sentence):\n",
        "    image_inputs = torch.stack([image_processor(Image.open(ipath)) for ipath in image_paths]).to(device)\n",
        "    text_input = clip.tokenize(sentence).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # compute embeddings\n",
        "        image_features = model.encode_image(image_inputs)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # normalize features\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # compute similarity scores\n",
        "    similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
        "\n",
        "    # rank images by similarity\n",
        "    probs, indices = similarity[0].topk(5)\n",
        "    return probs, indices"
      ],
      "metadata": {
        "id": "EWeSM7JpujrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_ranking_align(model, processor, image_paths, sentence):\n",
        "    image_inputs = [Image.open(ipath) for ipath in image_paths]\n",
        "    inputs = processor(images=image_inputs ,text=sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits_per_text = outputs.logits_per_text[0]\n",
        "    probs = logits_per_text.softmax(dim=-1)\n",
        "    ids_sorted = torch.argsort(probs, descending=True)\n",
        "    return probs[ids_sorted], ids_sorted"
      ],
      "metadata": {
        "id": "rCxWY_tFB0Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch\n",
        "import open_clip\n",
        "\n",
        "def openclip_image_ranking(model, image_processor, tokenizer, image_paths, sentence):\n",
        "    image_inputs = torch.stack([preprocess_openclip(Image.open(ipath)) for ipath in image_paths]).to(device)\n",
        "    text_input = tokenizer([sentence]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_inputs)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # normalise features\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # dot product & softmax\n",
        "    similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
        "\n",
        "    # order by similarity\n",
        "    probs, indices = similarity[0].topk(5)\n",
        "    return probs, indices"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TNOFQXLmNBQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e26d4c9-154d-4c4d-8be2-3638e51e8d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.10/dist-packages (2.29.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.20.1+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (2024.9.11)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.26.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (0.4.5)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open_clip_torch) (1.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open_clip_torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open_clip_torch) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open_clip_torch) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General functions for experiment execution"
      ],
      "metadata": {
        "id": "G-CBChVGeHoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, processor, image_paths_list, text_inputs, base_model, model_name):\n",
        "    \"\"\"\n",
        "    Uses get_image_ranking to generate predictions and confidence scores for a image-list, text-input pairs\n",
        "    \"\"\"\n",
        "    predictions, confidence_scores = [], []\n",
        "\n",
        "    if base_model == \"open_clip\":\n",
        "      tokenizer = open_clip.get_tokenizer(model_name)\n",
        "\n",
        "    for ipaths, text in zip(image_paths_list, text_inputs):\n",
        "        if len(ipaths) == 0:\n",
        "            predictions.append([])\n",
        "            confidence_scores.append([])\n",
        "            continue\n",
        "\n",
        "        # values, indices = get_image_ranking(ipaths, text)\n",
        "        if base_model == \"CLIP\":\n",
        "          values, indices = get_image_ranking_clip(model, processor, ipaths, text)\n",
        "        elif base_model == \"Align\":\n",
        "          values, indices = get_image_ranking_align(model, processor, ipaths, text)\n",
        "        elif base_model == \"open_clip\":\n",
        "          # values, indices = get_image_ranking_open_clip(model, processor, ipaths, text, model_name)\n",
        "          values, indices = openclip_image_ranking(model, processor, tokenizer, ipaths, text)\n",
        "        # elif base_model == \"BLIP\":\n",
        "        #   values, indices = get_image_ranking_blip(model, image_processor, ipaths, text)\n",
        "        else:\n",
        "          raise ValueError(f\"Unknown base_model: {base_model}\")\n",
        "        predictions.append(list(indices.cpu()))\n",
        "        confidence_scores.append(100 * values)\n",
        "\n",
        "    return predictions, confidence_scores\n"
      ],
      "metadata": {
        "id": "0KJzVPW5o-w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "def evaluate_predictions(predictions, df, weights=[0.4, 0.3, 0.2, 0.1, 0.0]):\n",
        "    \"\"\"\n",
        "    Takes predictions, returns three types of evaluation metrics:\n",
        "    - Top-1 Accuracy\n",
        "    - Average Spearman Correlation\n",
        "    - Average Weighted Accuracy\n",
        "    \"\"\"\n",
        "    correct_top1 = 0\n",
        "    spearman_scores, weighted_scores = [], []\n",
        "\n",
        "    for i in range(len(predictions)):\n",
        "        if len(predictions[i]) == 0:\n",
        "            continue\n",
        "\n",
        "        # Ground truth and predictions\n",
        "        pred_order = [df['image_idx_map'].iloc[i][os.path.basename(df['image_paths'].iloc[i][j])] for j in predictions[i]]\n",
        "        ground_truth_order = df['expected_order_indices'].iloc[i]\n",
        "\n",
        "        # Top-1 accuracy\n",
        "        if pred_order[0] == ground_truth_order[0]:\n",
        "            correct_top1 += 1\n",
        "\n",
        "        # Spearman correlation\n",
        "        score, _ = spearmanr(pred_order, ground_truth_order)\n",
        "        spearman_scores.append(score)\n",
        "\n",
        "        # Weighted accuracy\n",
        "        weighted_score = sum(weights[j] for j, img in enumerate(pred_order) if img == ground_truth_order[j])\n",
        "        weighted_scores.append(weighted_score)\n",
        "\n",
        "    return {\n",
        "        \"top1_accuracy\": correct_top1 / len(predictions),\n",
        "        \"average_spearman\": sum(spearman_scores) / len(spearman_scores),\n",
        "        \"average_weighted_accuracy\": sum(weighted_scores) / len(weighted_scores),\n",
        "        \"spearman_scores\": spearman_scores,\n",
        "        \"weighted_scores\": weighted_scores\n",
        "    }\n"
      ],
      "metadata": {
        "id": "EXSAb7fZpGD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_results(experiment_name, base_model, model_name, metrics, results_file=\"experiment_results.csv\"):\n",
        "    \"\"\"\n",
        "    Save experiment results to a CSV file.\n",
        "    \"\"\"\n",
        "    # Add experiment name to metrics\n",
        "    results_row = {\n",
        "        \"base_model\": base_model,\n",
        "        \"model\": model_name,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"top1_accuracy\": metrics[\"top1_accuracy\"],\n",
        "        \"average_spearman\": metrics[\"average_spearman\"],\n",
        "        \"average_weighted_accuracy\": metrics[\"average_weighted_accuracy\"],\n",
        "    }\n",
        "\n",
        "    # Write results to CSV\n",
        "    write_header = not os.path.exists(results_file)\n",
        "    with open(results_file, mode=\"a\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=results_row.keys())\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(results_row)\n",
        "\n",
        "    print(f\"Results saved to {results_file}\")"
      ],
      "metadata": {
        "id": "B4tJMjYjqG_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, processor, df, image_paths, text_inputs, model_name, experiment_name, base_model):\n",
        "    \"\"\"\n",
        "    Run an experiment using transformed embeddings, evaluate metrics, and print results.\n",
        "    \"\"\"\n",
        "    predictions, confidence_scores = get_predictions(model, processor, image_paths, text_inputs, base_model, model_name)\n",
        "    metrics = evaluate_predictions(predictions, df)\n",
        "    save_results(experiment_name, base_model, model_name, metrics)\n",
        "\n",
        "    print(f\"Top-1 Accuracy: {metrics['top1_accuracy'] * 100:.2f}%\")\n",
        "    print(f\"Average Spearman Correlation: {metrics['average_spearman']:.2f}\")\n",
        "    print(f\"Average Weighted Accuracy: {metrics['average_weighted_accuracy']:.2f}\")\n",
        "\n",
        "    return metrics, predictions, confidence_scores"
      ],
      "metadata": {
        "id": "q0ue0qdzK4zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def save_predictions(df, image_paths, predictions, confidence_scores, metrics, prefix, preds_dir='predictions'):\n",
        "    \"\"\"\n",
        "    Save detailed predictions and confidence scores for each example.\n",
        "    \"\"\"\n",
        "    # create 'preds' directory if doesn't exist\n",
        "    if not os.path.exists(preds_dir):\n",
        "        os.makedirs(preds_dir)\n",
        "\n",
        "    # generate output filename\n",
        "    prefix = prefix.strip().replace(\" \", \"_\")\n",
        "    prefix = re.sub(r'[^a-zA-Z0-9_-]', '', prefix)\n",
        "    output_path = f\"{preds_dir}/{prefix}_preds.csv\"\n",
        "\n",
        "    spearman_scores = metrics[\"spearman_scores\"]\n",
        "    weighted_scores = metrics[\"weighted_scores\"]\n",
        "    with open(output_path, mode=\"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"index\", \"compound\", \"ground_truth_order\", \"predicted_order\", \"top1_score\", \"spearman_score\", \"weighted_score\", \"confidence_scores\"])\n",
        "\n",
        "        for i, (pred, conf) in enumerate(zip(predictions, confidence_scores)):\n",
        "            pred_order = [df['image_idx_map'].iloc[i][os.path.basename(image_paths.iloc[i][j])] for j in pred]\n",
        "            ground_truth_order = df[\"expected_order_indices\"].iloc[i]\n",
        "            top1_score = 1 if pred_order[0] == ground_truth_order[0] else 0\n",
        "            spearman_score = round(spearman_scores[i], 3)\n",
        "            weighted_score = round(weighted_scores[i], 3)\n",
        "            formatted_conf_scores = [round(c.item(), 3) for c in conf]\n",
        "            writer.writerow([i, df[\"compound\"].iloc[i], ground_truth_order, pred_order, top1_score, spearman_score, weighted_score, formatted_conf_scores])\n",
        "\n",
        "    print(f\"Predictions saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "yzhmS1_kqc8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt GPT & populate data columns"
      ],
      "metadata": {
        "id": "aErtAe0NV-9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT Prompt 1: NC definition"
      ],
      "metadata": {
        "id": "yPbvQvgBUFsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Prompt #1\n",
        "\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# initialize openai client with colab secret key\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_PROJECT_KEY'))\n",
        "\n",
        "def generate_paraphrases_batched(compounds, sentence_types):\n",
        "    \"\"\"\n",
        "    Generate paraphrases in batches using GPT-4.\n",
        "    \"\"\"\n",
        "    # Filter out literals since they don't need processing\n",
        "    input_data = [\n",
        "        nc for nc, sentence_type in zip(compounds, sentence_types) if sentence_type != \"literal\"\n",
        "    ]\n",
        "    # Skip batch if all are literal\n",
        "    if not input_data:\n",
        "        return {nc: nc for nc in compounds}  # Return original NCs for all literals\n",
        "\n",
        "    # Create a combined prompt\n",
        "    examples = \"\\n\".join([\n",
        "        f'The idiom is: \"{nc}\".' for nc in input_data\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a linguistics expert specializing in idioms. For each of the idioms below, provide a definition that balances accuracy with its emotional and cultural essence.\n",
        "\n",
        "Example:\n",
        "Idiom: \"cold turkey\"\n",
        "Definition: \"Abruptly quitting a habit, marked by discomfort and determination.\"\n",
        "\n",
        "Idioms to define:\n",
        "{examples}\n",
        "\n",
        "Respond in this format:\n",
        "{{\"idioms\": [\n",
        "    {{\"idiom\": \"cold turkey\", \"definition\": \"Abruptly quitting a habit, marked by discomfort and determination.\"}},\n",
        "    {{\"idiom\": \"another idiom\", \"definition\": \"Its definition here.\"}}\n",
        "]}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        content = json.loads(response.choices[0].message.content.strip())\n",
        "        results = {item[\"idiom\"]: item[\"definition\"] for item in content[\"idioms\"]}\n",
        "        for idiom, definition in results.items():\n",
        "            print(f\"Idiom: {idiom}\\nDefinition: {definition}\\n\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating paraphrases: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "MlihCIXK3jn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to the DataFrame in batches\n",
        "batch_size = 10\n",
        "results = {}\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch = df.iloc[i:i + batch_size]\n",
        "    compounds = batch['compound'].tolist()\n",
        "    sentence_types = batch['sentence_type'].tolist()\n",
        "    batch_results = generate_paraphrases_batched(compounds, sentence_types)\n",
        "    results.update(batch_results)\n",
        "\n",
        "# Map results back to DataFrame\n",
        "df['gpt_prompt_1_resp'] = df['compound'].map(results)\n",
        "\n",
        "# For rows with null 'paraphrased_nc', set equal to 'compound'\n",
        "df['gpt_prompt_1_resp'].fillna(df['compound'], inplace=True)"
      ],
      "metadata": {
        "id": "F-cpXwfz3o93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e24b759-6edc-4c83-f78d-ca9f6e23970d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Idiom: elbow grease\n",
            "Definition: Hard physical work, especially vigorous cleaning, characterized by personal effort and perseverance.\n",
            "\n",
            "Idiom: night owl\n",
            "Definition: A person habitually active or awake at night, often associated with creativity or solitude.\n",
            "\n",
            "Idiom: heart of gold\n",
            "Definition: Incredibly kind and generous nature, reflecting a person's ability to be compassionate and unselfish.\n",
            "\n",
            "Idiom: agony aunt\n",
            "Definition: A person, often a columnist, who gives advice to people with personal problems, embodying sensitivity and wisdom.\n",
            "\n",
            "Idiom: shrinking violet\n",
            "Definition: A person who is shy or modest and avoids drawing attention to themselves, often associated with introversion or lack of confidence.\n",
            "\n",
            "Idiom: banana republic\n",
            "Definition: A small nation, especially in the tropics, dependent on one crop or the influx of foreign capital, often associated with instability, corruption, and inequality.\n",
            "\n",
            "Idiom: private eye\n",
            "Definition: A private investigator, often operating outside law enforcement, symbolizing an individual's pursuit of truth and justice.\n",
            "\n",
            "Idiom: cold turkey\n",
            "Definition: Abruptly quitting a habit, marked by discomfort and determination.\n",
            "\n",
            "Idiom: pipe dream\n",
            "Definition: A wish or idea that is unrealistic or fanciful, often marked by an element of hope and disappointment.\n",
            "\n",
            "Idiom: rocket science\n",
            "Definition: An activity or concept that is very complex or difficult to understand, often used humorously to overstate the complexity of mundane tasks.\n",
            "\n",
            "Idiom: nest egg\n",
            "Definition: A sum of money saved for the future, often with a sense of security and foresight.\n",
            "\n",
            "Idiom: bull market\n",
            "Definition: A period of rising stock prices, often associated with optimism and investor confidence.\n",
            "\n",
            "Idiom: beached whale\n",
            "Definition: A person or thing that is out of place or in a situation where they do not fit in, often used humorously or self-deprecatingly.\n",
            "\n",
            "Idiom: cold turkey\n",
            "Definition: Abruptly quitting a habit, marked by discomfort and determination.\n",
            "\n",
            "Idiom: lounge lizard\n",
            "Definition: A person, typically a man, who spends a lot of time in social settings, often in order to seduce others.\n",
            "\n",
            "Idiom: bear market\n",
            "Definition: A market situation in which prices are falling, creating a sense of pessimism and caution among investors.\n",
            "\n",
            "Idiom: white hat\n",
            "Definition: In the context of cybersecurity, a person who hacks into systems to find and fix vulnerabilities, serving a noble and protective role.\n",
            "\n",
            "Idiom: cold turkey\n",
            "Definition: Abruptly quitting a habit, marked by discomfort and determination.\n",
            "\n",
            "Idiom: smoking gun\n",
            "Definition: Irrefutable evidence that proves guilt or wrongdoing, suggestive of a dramatic disclosure.\n",
            "\n",
            "Idiom: old flame\n",
            "Definition: A former romantic partner, whose memory often evokes nostalgia and longing.\n",
            "\n",
            "Idiom: ivory tower\n",
            "Definition: A place or state of privileged seclusion, often related to academic or artistic pursuits, disconnected from practical realities.\n",
            "\n",
            "Idiom: black sheep\n",
            "Definition: An individual who differs significantly from their family or group, often viewed with disapproval or as an outcast.\n",
            "\n",
            "Idiom: gravy train\n",
            "Definition: A highly lucrative and relatively easy way of earning money, often seen as undeserved or unethical.\n",
            "\n",
            "Idiom: rat race\n",
            "Definition: A competitive struggle for success or survival within a structured environment, often characterized by stress and endless pursuit of material wealth.\n",
            "\n",
            "Idiom: spring chicken\n",
            "Definition: Used to represent someone young, energetic, often naive, among a group of significantly older or experienced persons.\n",
            "\n",
            "Idiom: inner circle\n",
            "Definition: A small, intimate, and influential group of people who share interests and confidences, and exert control over a larger group. This idiom reflects exclusivity and power dynamics.\n",
            "\n",
            "Idiom: bad apple\n",
            "Definition: One person within a group who, because of their negative behaviour, influences or spoils the entire group. This idiom underscores the impact of individual influence.\n",
            "\n",
            "Idiom: honey trap\n",
            "Definition: A strategic scenario where an attractive person lures another into revealing important information or into a compromising situation, underscoreing manipulation by appeal.\n",
            "\n",
            "Idiom: open book\n",
            "Definition: An individual or situation that is easy to understand or discern, indicative of transparency.\n",
            "\n",
            "Idiom: baby blues\n",
            "Definition: Mild depression, sadness, or worry typically experienced shortly after childbirth, infused with tender concern and vulnerability.\n",
            "\n",
            "Idiom: brain surgery\n",
            "Definition: A task, issue, or problem that is highly complex or difficult, often requiring intricate precision or knowledge.\n",
            "\n",
            "Idiom: red flag\n",
            "Definition: An indication or warning of a problem, danger, or impending disaster, demanding immediate attention and reaction.\n",
            "\n",
            "Idiom: white elephant\n",
            "Definition: A possession which its owner cannot dispose of and whose cost (particularly cost of upkeep) is out of proportion to its usefulness or worth, colored by frustration and regret.\n",
            "\n",
            "Idiom: rat run\n",
            "Definition: A minor, often residential street used by drivers during heavy traffic to bypass main roads, connotating opportunism and evasion.\n",
            "\n",
            "Idiom: graveyard shift\n",
            "Definition: A work shift running through the late hours of the night until early morning, carrying undertones of endurance and isolation.\n",
            "\n",
            "Idiom: dirty money\n",
            "Definition: Wealth obtained through illegal or unethical actions, stained by guilt and secrecy.\n",
            "\n",
            "Idiom: high life\n",
            "Definition: Living in luxury and indulgence, often showcasing wealth and status.\n",
            "\n",
            "Idiom: guinea pig\n",
            "Definition: Someone or something used in a test or experiment, echoing the vulnerability of being a test subject.\n",
            "\n",
            "Idiom: cat's eyes\n",
            "Definition: Witnessing or observing discreetly, invoking the stealth and sharp observation of a cat.\n",
            "\n",
            "Idiom: low-hanging fruit\n",
            "Definition: Easily achievable tasks or goals, reflecting the simple effort it takes to pick fruit hanging low on a tree.\n",
            "\n",
            "Idiom: busy bee\n",
            "Definition: An individual who is hardworking and consistently active, reflecting the industrious nature of a bee.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-36673918767d>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['gpt_prompt_1_resp'].fillna(df['compound'], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT Prompt 2: NC definition with multi-step reasoning"
      ],
      "metadata": {
        "id": "LKjGmZARTLs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Prompt #2\n",
        "\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# initialize openai client with colab secret key\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_PROJECT_KEY'))\n",
        "\n",
        "def generate_paraphrases_batched_2(compounds, sentence_types):\n",
        "    \"\"\"\n",
        "    Generate paraphrases in batches using GPT-4.\n",
        "    \"\"\"\n",
        "    # Filter out literals since they don't need processing\n",
        "    input_data = [\n",
        "        nc for nc, sentence_type in zip(compounds, sentence_types) if sentence_type != \"literal\"\n",
        "    ]\n",
        "    # Skip batch if all are literal\n",
        "    if not input_data:\n",
        "        return {nc: nc for nc in compounds}  # Return original NCs for all literals\n",
        "\n",
        "    # Create a combined prompt\n",
        "    examples = \"\\n\".join([\n",
        "        f'The idiom is: \"{nc}\".' for nc in input_data\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a linguistics expert specializing in idioms. For each of the idioms below, do the following steps aloud (in writing):\n",
        "1. Give a verbose explanation of the idiom, including what connotations it carries or undertones it evokes.\n",
        "2. List three potential definitions, no longer than 20 words each, that capture the essence of the phrase in a general manner.\n",
        "3. Choose the best definition.\n",
        "\n",
        "Example #1:\n",
        "Idiom: \"cold turkey\"\n",
        "Definition: \"Abruptly quitting a habit or addiction, overcoming discomfort or pain, requiring determination.\"\n",
        "\n",
        "Example #2:\n",
        "Idiom: \"piece of cake\"\n",
        "Definition: \"Easy, simple to accomplish, requiring little effort, not a problem.\"\n",
        "\n",
        "Idioms to define:\n",
        "{examples}\n",
        "\n",
        "Respond in this format:\n",
        "{{\"idioms\": [\n",
        "    {{\"idiom\": \"cold turkey\", \"verbose_definition\": \"<verbose_definition>\", \"possible_definitions\": \"<three possible definitions>\", \"definition\": \"Abruptly quitting a habit, marked by discomfort and determination.\"}},\n",
        "    {{\"idiom\": \"<another idiom>\", \"verbose_definition\": \"<verbose_definition>\", \"possible_definitions\": \"<three possible definitions>\" , \"definition\": \"<its definition>\" }}\n",
        "]}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        content = json.loads(response.choices[0].message.content.strip())\n",
        "        results = {item[\"idiom\"]: item[\"definition\"] for item in content[\"idioms\"]}\n",
        "        for item in content[\"idioms\"]:\n",
        "            print(f\"Idiom: {item['idiom']}\\nVerbose: {item['verbose_definition']}\\nPossible: {item['possible_definitions']}\\nDefinition: {item['definition']}\\n\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating paraphrases: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "SapRs6r8Xe3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to the DataFrame in batches\n",
        "batch_size = 10\n",
        "results = {}\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch = df.iloc[i:i + batch_size]\n",
        "    compounds = batch['compound'].tolist()\n",
        "    sentence_types = batch['sentence_type'].tolist()\n",
        "    batch_results = generate_paraphrases_batched_2(compounds, sentence_types)\n",
        "    results.update(batch_results)\n",
        "\n",
        "# Map results back to DataFrame\n",
        "df['gpt_prompt_2_resp'] = df['compound'].map(results)\n",
        "\n",
        "# For rows with null 'paraphrased_nc', set equal to 'compound'\n",
        "df['gpt_prompt_2_resp'].fillna(df['compound'], inplace=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kb5mivD2aMD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8af9af-7392-416b-b763-b4963a14fe54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Idiom: elbow grease\n",
            "Verbose: Hard work and determination applied to a menial or difficult task, often referring to a physically demanding one. The idiom evokes the image of putting in extra effort or exertion.\n",
            "Possible: ['Applying hard work and commitment.', 'Effort used in scrubbing or cleaning.', 'Physical exertion done determinedly.']\n",
            "Definition: Applying hard work and commitment.\n",
            "\n",
            "Idiom: night owl\n",
            "Verbose: Refers to a person who is naturally most alert, productive, or creative during the night or late hours. The phrase evokes the nocturnal habits of the owl.\n",
            "Possible: ['Individual who stays up late.', 'Person preferring late hours for productivity.', 'Someone active during night-time.']\n",
            "Definition: Person preferring late hours for productivity.\n",
            "\n",
            "Idiom: heart of gold\n",
            "Verbose: Describes someone who is genuinely kind, compassionate, or selfless, much like the way gold is valued for its worth. This idiom suggests selflessness and moral worth.\n",
            "Possible: ['Being kind and generous.', 'Characterized by selflessness or compassion.', 'Genuinely good-natured.']\n",
            "Definition: Being kind and generous.\n",
            "\n",
            "Idiom: agony aunt\n",
            "Verbose: Refers to a person, often a woman, providing advice or listening empathetically to other people's problems. Traditionally, it's connected to advice columns in newspapers or magazines.\n",
            "Possible: ['Person who offers advice or solutions.', 'Advice columnist in media.', \"Listener and advisor for others' problems.\"]\n",
            "Definition: Person who offers advice or solutions.\n",
            "\n",
            "Idiom: shrinking violet\n",
            "Verbose: This idiom refers to a person who is excessively shy, introverted, or who avoids drawing attention to themselves. The term brings to mind the image of a delicate violet flower retracting its petals.\n",
            "Possible: ['Excessively shy individual.', 'Person avoiding attention or spotlight.', 'Introverted or timid person.']\n",
            "Definition: Excessively shy individual.\n",
            "\n",
            "Idiom: banana republic\n",
            "Verbose: A pejorative term for a small country that is politically unstable and whose economy is dominated by foreign companies. Originating from the exploitation of Central American countries by banana companies.\n",
            "Possible: ['Politically unstable small country.', 'Country dependent on foreign corporations.', 'State exploited by foreign companies.']\n",
            "Definition: Politically unstable small country.\n",
            "\n",
            "Idiom: private eye\n",
            "Verbose: Refers to a private investigator or a detective who is not part of a police force. It's associated with the traditional image of a sleuth or detective.\n",
            "Possible: ['Private detective or investigator.', 'Non-police detective.', 'Independent investigator.']\n",
            "Definition: Private detective or investigator.\n",
            "\n",
            "Idiom: pipe dream\n",
            "Verbose: A pipe dream is an aspiration or goal that is perceived as unrealistically optimistic or fanciful. The idiom derives from the 19th century when opium smokers would have vivid hallucinations, thus their dreams or ambitions while under the influence were known as 'pipe dreams.' It often has a connotation of delusion or naivety.\n",
            "Possible: ['Unrealistic or fanciful goal', 'Unattainable desire or aspiration', 'Dream that is unlikely to be fulfilled']\n",
            "Definition: Unrealistic or fanciful goal.\n",
            "\n",
            "Idiom: rocket science\n",
            "Verbose: The term 'rocket science' is used metaphorically to suggest something requiring great intelligence or technical know-how. It draws allusion from the actual field of rocket science, which involves complex principles of physics and engineering. Hence, it implies a task or concept being highly difficult or overly complicated.\n",
            "Possible: ['Extremely difficult task', 'Concept requiring high intelligence', 'Overly complicated idea']\n",
            "Definition: Extremely difficult task.\n",
            "\n",
            "Idiom: nest egg\n",
            "Verbose: The term 'nest egg' originates from the practice of leaving an egg in a nest to encourage a hen to continue laying. Figuratively, it refers to a sum of money saved for the future, typically for retirement. It carries the undertones of prudence, patience, and long-term planning.\n",
            "Possible: ['Retirement savings', 'Money saved for future use', 'Funds accumulated for security']\n",
            "Definition: Money saved for future use.\n",
            "\n",
            "Idiom: bull market\n",
            "Verbose: The term 'bull market' is from financial jargon, it refers to a market scenario where prices are expected to rise or are rising, providing an opportunity for investors to buy. The bull symbolizes charging ahead with force, suggesting the aggressive and optimistic nature of such a market condition.\n",
            "Possible: ['Market with rising prices', 'Favorable investment condition', 'Economic situation indicating growth']\n",
            "Definition: Market with rising prices.\n",
            "\n",
            "Idiom: beached whale\n",
            "Verbose: A 'beached whale' is a term used metaphorically to refer to someone or something that is out of place or in a situation where it doesn't belong. Just as a whale stranded on a beach is in a dire circumstance, the term carries connotations of discomfort, helplessness, or conspicuousness.\n",
            "Possible: ['Someone out of place', 'Thing in an unsuitable situation', 'Figuratively stranded']\n",
            "Definition: Someone out of place.\n",
            "\n",
            "Idiom: lounge lizard\n",
            "Verbose: The term 'lounge lizard' is often used pejoratively to describe a man who frequents public venues, such as nightclubs or bars, with the intention of seducing women. The idiom carries connotations of insincerity, manipulation, and predatory behaviour. It may evoke an image of a man who is not sincerely interested in genuine relationships with women, but instead seeks to charm them for his own benefit.\n",
            "Possible: ['A man who hangs out in public venues aiming to charm women', 'An insincere man seducing women in clubs or bars', 'A man frequenting nightspots to woo women insincerely']\n",
            "Definition: An insincere man seducing women in clubs or bars.\n",
            "\n",
            "Idiom: bear market\n",
            "Verbose: A 'bear market' is a term used in the financial world to describe a situation in which the prices of securities are falling, and widespread pessimism causes the negative sentiment to be self-sustaining. It is often associated with a slowdown in the economy and is characterized by investors' lack of confidence. The bear, seen as a creature that moves slowly and cautiously, is used as a symbol for such market conditions.\n",
            "Possible: ['A financial market with falling prices', 'An economic period of pessimism and declining securities value', 'A state in trading where stock prices fall over an extended time']\n",
            "Definition: A financial market with falling prices.\n",
            "\n",
            "Idiom: white hat\n",
            "Verbose: The idiom 'white hat' originates from Western movies where heroes would wear white cowboy hats, in contrast to villains who wore black ones. In the world of cyber security, a 'white hat' is a hacker who uses their skills for good by helping to discover and fix security vulnerabilities. This idiom carries positive connotations of helpfulness, ethical behavior, and working within the law to improve security.\n",
            "Possible: ['A hacker who improves security systems', 'An ethical hacker working for good purposes', 'Positive and helpful cybersecurity expert']\n",
            "Definition: An ethical hacker working for good purposes.\n",
            "\n",
            "Idiom: smoking gun\n",
            "Verbose: This idiom originated in the context of a smoking firearm, acting as a surefire piece of evidence or clue that indicates guilt in a crime investigation. It has evolved to represent any form of irrefutable proof or clear demonstration of allegation\n",
            "Possible: ['Strong, undeniable proof', 'Clear evidence of guilt', 'Indisputable indication of a crime']\n",
            "Definition: Clear evidence of guilt.\n",
            "\n",
            "Idiom: old flame\n",
            "Verbose: The metaphor 'old flame' may elicit vivid visuals of a once-bright, passionate fire that has since faded but can be reignited. It is a nostalgic term used to denote a former lover, someone with whom the speaker had a significant romantic relationship\n",
            "Possible: ['A previous romantic partner', 'An individual once in a meaningful loving relationship', 'Former love interest']\n",
            "Definition: A previous romantic partner.\n",
            "\n",
            "Idiom: ivory tower\n",
            "Verbose: The 'ivory tower' signifies a privileged, sometimes disconnected, state of being, often related to academia or scholars who are seemingly detached from real-world issues. It is a place identified as being removed from the practical world, filled with metaphorically lofty and detached intellectuals\n",
            "Possible: ['An isolated place of intellectualism', 'State of being disconnected from practical realities', 'Refuge of the privileged, often removed from the worldly troubles']\n",
            "Definition: State of being disconnected from practical realities.\n",
            "\n",
            "Idiom: black sheep\n",
            "Verbose: In 'black sheep', the color black is used to indicate difference or deviance, while sheep is a common symbol for a group or flock. Thus, a black sheep is someone who noticeably doesn't fit into their family or group due to their unconventional behavior or habits\n",
            "Possible: ['An outcast within a group', 'Someone who stands out due to differences', \"A person who doesn't conform to the expectations\"]\n",
            "Definition: An outcast within a group.\n",
            "\n",
            "Idiom: gravy train\n",
            "Verbose: The 'gravy train' essentially describes a situation or occupation where one can make substantial amounts of money with little effort. The phrase evokes undertones of windfall bonuses, luxurious livelihoods, and easy wealth\n",
            "Possible: ['Source of easy money', 'Profitable endeavour involving minimal effort', 'Situation that leads to substantial financial gain']\n",
            "Definition: Source of easy money.\n",
            "\n",
            "Idiom: rat race\n",
            "Verbose: The 'rat race' is often visualized as a circle of rats endlessly chasing something, reminiscent of the daily grind in human lives. It commonly refers to the endless, self-defeating and unrewarding pursuit of wealth or higher societal position\n",
            "Possible: ['Endless, frustrating pursuit for success', 'Futile chase for material wealth', 'Non-stop but unrewarding struggle for progress']\n",
            "Definition: Endless, frustrating pursuit for success.\n",
            "\n",
            "Idiom: spring chicken\n",
            "Verbose: The idiom 'spring chicken' refers to a person who is young and in good health. The term originates from the preference for young, spring-born chickens which are more tender than older ones. By extension, it is often used in a slightly ironic or humorous way to refer to somebody who is not as young as they once were.\n",
            "Possible: ['A person who is youthful and vigorous', 'Someone who is immature due to their youth', 'An individual in their prime years of life']\n",
            "Definition: A person who is youthful and vigorous\n",
            "\n",
            "Idiom: inner circle\n",
            "Verbose: The 'inner circle' refers to an exclusive group of people in a society or organization who hold power and influence, often close to a central figure of authority. Membership in this group typically signifies trust, confidence, and privileged status. The feelings evoked by this idiom can include exclusivity, power, secrecy and influence.\n",
            "Possible: ['A select group of influential individuals', 'Confidants or close associates of a person', 'Influential people with privileged access to information']\n",
            "Definition: A select group of influential individuals\n",
            "\n",
            "Idiom: bad apple\n",
            "Verbose: A 'bad apple' is an idiom describing a person who negatively influences others within a group, or an individual known for their bad behavior or poor character. It evokes the old adage: 'one bad apple spoils the barrel' suggesting that the negatives can often overshadow the positives, thus can bring down the morale of the entire group.\n",
            "Possible: ['A troublesome person within a group', 'An individual with bad character or behavior', 'A person causing issues within a larger community']\n",
            "Definition: An individual with bad character or behavior\n",
            "\n",
            "Idiom: honey trap\n",
            "Verbose: A 'honey trap' is an idiom that denotes a strategic encounter designed to deceive or trap someone, often using temptations appealing to their personal instincts. Typically found in the context of espionage, it implies seduction for manipulation or betrayal. The undertones of the phrase are associated with deceit, manipulation, and betrayal.\n",
            "Possible: ['A deceptive act designed to trap or ensnare', 'Seductive bait in a strategic scheme', 'Allure used to deceive or lead astray']\n",
            "Definition: A deceptive act designed to trap or ensnare\n",
            "\n",
            "Idiom: open book\n",
            "Verbose: The idiom 'open book' is often used to describe a person who is easy to understand or read. They usually lack secrecy or hidden aspects in their character, emotions or thoughts. They are candid, straightforward, and hide nothing, just like an open book where every detail is displayed evidently.\n",
            "Possible: ['Someone who is straightforward', 'A person with no secrets', 'An uncomplicated individual']\n",
            "Definition: A person who is easy to understand or read, without secrecy or hidden aspects.\n",
            "\n",
            "Idiom: baby blues\n",
            "Verbose: The idiom 'baby blues' typically refers to a state of mild, transient emotional distress experienced by new mothers in the days following childbirth due to hormonal changes and physical exhaustion. In a broader context, it can also depict sadness or depression.\n",
            "Possible: ['Emotional distress after childbirth', 'State of mild depression', 'Temporary sadness in new mothers']\n",
            "Definition: Mild, transient emotional distress experienced by new mothers after childbirth.\n",
            "\n",
            "Idiom: brain surgery\n",
            "Verbose: The idiom 'brain surgery' is often used to emphasize the extreme difficulty or complexity of a task. The phrase derives from the actual profession which requires high skill and knowledge, and is thus metaphorically used to denote any situation that's complex or challenging.\n",
            "Possible: ['A very difficult task', 'Complicated procedure', 'Task requiring great skill']\n",
            "Definition: A task or situation that is extremely complex or difficult.\n",
            "\n",
            "Idiom: red flag\n",
            "Verbose: The idiom 'red flag' traditionally symbolizes danger or a warning sign. It's used as a metaphor to indicate potential problems or trouble that require attention. It is often linked with situations where caution should be exercised.\n",
            "Possible: ['A warning sign', 'Indicator of a problem', 'Sign of danger or trouble']\n",
            "Definition: A warning sign indicating potential problems or trouble.\n",
            "\n",
            "Idiom: white elephant\n",
            "Verbose: A 'white elephant' is a phrase that describes something that is costly and hard to maintain but is also difficult to get rid of. It comes from historical practices where kings used to gift a white elephant, a burdensome yet sacred creature, which was expensive to sustain.\n",
            "Possible: ['A burdensome possession', 'Expensive item that is hard to dispose of', 'Costly and inconvenient asset']\n",
            "Definition: An expensive and burdensome possession that's hard to get rid of.\n",
            "\n",
            "Idiom: rat run\n",
            "Verbose: The term 'rat run' refers to a minor, often residential street used by drivers during peak periods to avoid traffic on main roads. It gives the notion of scurrying rats in a maze, ensuring a quick escape route from one place to another.\n",
            "Possible: ['Shortcut through minor streets', 'Alternative route to avoid traffic', 'A quick, less crowded driving path']\n",
            "Definition: A minor, often residential, street used as a shortcut to avoid traffic.\n",
            "\n",
            "Idiom: graveyard shift\n",
            "Verbose: The phrase 'graveyard shift' is used to denote late night or overnight shift work. Often associated with jobs that require workers to stay active during the night hours, it evokes a sense of loneliness and eeriness typically associated with graveyards at night.\n",
            "Possible: ['Overnight work schedule', 'Working during the late night hours', 'Night shift work']\n",
            "Definition: Late night or overnight shift work.\n",
            "\n",
            "Idiom: dirty money\n",
            "Verbose: The idiom 'dirty money' refers to currency or wealth that has been obtained in unethical or illegal ways, evoking the immorality and guilt associated with such gains.\n",
            "Possible: ['Money obtained through illegal or immoral activities', 'Illicitly earned funds', 'Unethically gained wealth']\n",
            "Definition: Money obtained through illegal or immoral activities.\n",
            "\n",
            "Idiom: high life\n",
            "Verbose: The phrase 'high life' refers to a luxurious or extravagant lifestyle, often associated with wealth, indulgence, and leisure.\n",
            "Possible: ['Living luxuriously and extravagantly', 'Affluent, indulgent lifestyle', 'Wealthy and extravagant way of living']\n",
            "Definition: Living luxuriously and extravagantly.\n",
            "\n",
            "Idiom: guinea pig\n",
            "Verbose: The term 'guinea pig' refers to anyone used as a subject for experimentation or testing. It implies a situation where there could be risks or unknown outcomes for the person involved.\n",
            "Possible: ['Individual used for testing', 'Subject of an experiment', \"One who undergoes trials for others' benefit\"]\n",
            "Definition: Individual used for testing.\n",
            "\n",
            "Idiom: cat's eyes\n",
            "Verbose: The idiom 'cat's eyes' usually refers to the reflective road safety devices found on many highways. It can also refer to a watchful, sharp gaze.\n",
            "Possible: ['Road safety reflectors', 'Intensely watchful gaze', 'Sharp, observant eyes']\n",
            "Definition: Road safety reflectors or an intensely watchful gaze.\n",
            "\n",
            "Idiom: low-hanging fruit\n",
            "Verbose: The term 'low-hanging fruit' typically refers to tasks, goals, or objectives that can be achieved with minimal effort or difficulty. It implies an opportunity for easy gains or progress.\n",
            "Possible: ['Easy-to-reach goals or targets', 'Tasks that require minimal effort', 'Readily achievable objectives']\n",
            "Definition: Easy-to-reach goals or targets.\n",
            "\n",
            "Idiom: busy bee\n",
            "Verbose: The idiom 'busy bee' is a metaphorical phrase used to describe someone who is energetic and hardworking, often constantly busy or always involved in numerous activities. It carries a positive connotation and suggests industriousness and diligence.\n",
            "Possible: ['Person who is very active and industrious', 'Hardworking, energetic individual', 'Someone constantly busy with activities']\n",
            "Definition: Person who is very active and industrious.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-837785a48e1d>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['gpt_prompt_2_resp'].fillna(df['compound'], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chain of thought variant"
      ],
      "metadata": {
        "id": "5DjXnXrGt_YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Prompt #4\n",
        "\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# initialize openai client with colab secret key\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_PROJECT_KEY'))\n",
        "\n",
        "def generate_paraphrases_4(compound, sentence):\n",
        "    \"\"\"\n",
        "    Generate paraphrases using GPT-4.\n",
        "    \"\"\"\n",
        "    # Create a combined prompt\n",
        "    # examples = \"\\n\".join([\n",
        "    #     f'Phrase: \"{nc}\"\\nSentence: \"{sent}\"\\n' for nc, sent in zip(compounds, sentences)\n",
        "    # ])\n",
        "    prompt = f\"\"\"You are a linguistics expert specializing in idioms. Given a potentially idiomatic phrase, and a sentence containing that phrase, you should determine whether the phrase is used literally or idiomatically in this particular instance, and describe the meaning of the phrase in this context.\n",
        "Explain your reasoning.\n",
        "\n",
        "Example #1\n",
        "Phrase: \"kick the bucket\"\n",
        "Sentence: \"As John stood up from the table he accidentally kicked the bucket that was hidden underneath it.\"\n",
        "Reasoning: To kick is to strike with the foot. A bucket is a container made of metal or plastic for carrying water. Kicking a bucket therefore means striking a container with the foot.\n",
        "Alternatively, to kick the bucket, when used idiomatically, can mean to die.\n",
        "Which option makes more sense in this case?\n",
        "In the literal case, the sentence would mean that John is kicking a physical object, which we are told is underneath the table. It is plausible that a bucket could be hidden underneath a table and that standing up would cause one to kick that bucket.\n",
        "In the idiomatic case, the sentence would mean that John accidentally dies as he stands up from the table, but then what is hidden underneath the table? The action of dying cannot be underneath a table. This usage does not make sense.\n",
        "Therefore, the phrase is used literally in this instance.\n",
        "Example 1 response: {{\"phrase\": \"kick the bucket\", \"usage\": \"literal\", \"meaning\": \"kicking a bucket, which might cause pain in the foot or cause a loud noise.\"}}\n",
        "\n",
        "\n",
        "Example #2\n",
        "Idiom: \"piece of cake\"\n",
        "Sentence: \"Sarah thought the final exam was a piece of cake compared to the midterm.\"\n",
        "Reasoning: A cake is a baked dessert, a piece of cake is a small amount of desert.\n",
        "Alternatively, in idiomatic usage, something is considered a piece of cake if it is easy or requires little effort.\n",
        "Which option makes more sense in this case?\n",
        "In the literal case, the sentence would mean that Sarah thinks her exam is a piece of dessert. The piece of cake is compared to a different exam. It is unlikely that an exam is literally a piece of cake, and cake is not a comparable value like distance or size or intensity. This usage is unlikely.\n",
        "In the idiomatic case, the sentence would mean that Sarah thinks her final exam was easy compared to the midterm. It is common to compare one exam with another in terms of its difficulty, so this usage makes sense.\n",
        "Therefore, the phrase is used idiomatically in this instance.\n",
        "Example 2 response: {{\"phrase\": \"piece of cake\", \"usage\": \"idiomatic\", \"meaning\": \"an activity or task that is delightfully easy, requiring little effort and quickly accomplished.\"}}\n",
        "\n",
        "\n",
        "Example #3\n",
        "Phrase: \"{compound}\"\n",
        "Sentence: \"{sentence}\"\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        pos = response.choices[0].message.content.strip().find('Example 3 response')\n",
        "        json_response = response.choices[0].message.content.strip()[pos+19:]\n",
        "        content = json.loads(json_response)\n",
        "        print(f\"Idiom: {content['phrase']}\\nMeaning: {content['meaning']}\\nUsage: {content['usage']}\")\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating paraphrases: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "a56QgBTbt-XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to the DataFrame in batches\n",
        "results = []\n",
        "for i in range(len(df)):\n",
        "    batch = df.iloc[i]\n",
        "    compound = batch['compound']\n",
        "    sentence = batch['sentence']\n",
        "    batch_results = generate_paraphrases_4(compound, sentence)\n",
        "    results.append(batch_results)\n",
        "\n",
        "# # Map results back to DataFrame\n",
        "df['gpt_prompt_4_resp'] = [res['meaning'] for res in results]\n",
        "df['gpt_prompt_4_type'] = [res['usage'] for res in results]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SRqimI_Mt-Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('gpt_prompt_4_results.csv', mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "#     fieldnames = results[0].keys()\n",
        "#     writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "#     writer.writeheader()\n",
        "#     writer.writerows(results)"
      ],
      "metadata": {
        "id": "rlrANORSPt2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['gpt_prompt_4_correct_type'] = df['gpt_prompt_4_type'] == df['sentence_type']"
      ],
      "metadata": {
        "id": "9SQZtsgYO0mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['gpt_prompt_4_correct_type'].sum()"
      ],
      "metadata": {
        "id": "bOrzDNFLO6iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT Prompt 3: NC in-context definition, prompt decomposition (subtasks)"
      ],
      "metadata": {
        "id": "uZgJsl5kTP6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Prompt #3\n",
        "\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# initialize openai client with colab secret key\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_PROJECT_KEY'))\n",
        "\n",
        "def generate_paraphrases_batched_3(compounds, sentences):\n",
        "    \"\"\"\n",
        "    Generate paraphrases in batches using GPT-4.\n",
        "    \"\"\"\n",
        "    # Create a combined prompt\n",
        "    examples = \"\\n\\n\".join([\n",
        "        f'Phrase: \"{nc}\"\\nSentence: \"{sentence}\"' for nc, sentence in zip(compounds, sentences)\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a linguistics expert specializing in figurative language. You will be given a set samples, each containing a potentially figurative English phrase paired with a sentence that said phrase is used in.\n",
        "For each sample, you are to do the following:\n",
        "1. Read the sentence; consider how the phrase is used in the sentence. It might be used figuratively (i.e. as an idiom), and it might be used literally (i.e. word composition).\n",
        "2. Verbose explanation: Given your familiarity with the phrase's possible meanings, and having considered how it's used in the sentence, give a verbose explanation of what the phrase means in the context of the sentence. This can be a few sentences long.\n",
        "3. Determine usage: State whether the phrase is used figuratively or literally in the sentence.\n",
        "4. Definition: A concise, generalized definition of the phrase in this sentence.\n",
        "5. Other usage definition: Assuming the phrase has both a literal definition and a figurative definition, give the definition for the OTHER usage.\n",
        "\n",
        "Example #1:\n",
        "<Sample>\n",
        "Phrase: \"cold turkey\"\n",
        "Sentence: \"John quit smoking cold turkey and never looked back, not that it was easy.\"\n",
        "---\n",
        "<Output>\n",
        "#3 - \"Figuratively\"\n",
        "#4 - \"Abruptly quitting a habit, marked by discomfort and determination\"\n",
        "#5 - \"A turkey, which is a type of bird, that is cold\"\n",
        "\n",
        "Example #2:\n",
        "<Sample>\n",
        "Phrase: \"piece of cake\"\n",
        "Sentence: \"The boy eyed the piece of cake from afar as the waitress approached from across the room.\"\n",
        "---\n",
        "<Output>\n",
        "#3 - \"Literally\"\n",
        "#4 - \"A slice of a sweet, baked dessert\"\n",
        "#5 - \"A task that can be completed with no difficulty; something easy to accomplish\"\n",
        "\n",
        "Respond in this format:\n",
        "{{\"samples\": [\n",
        "    {{\"phrase\": \"cold turkey\", \"verbose_definition\": \"<see #2 above>\", \"usage\": \"<see #3 above>\", \"definition\": \"<see #4 above>\", \"other_definition\": \"<see #5 above>\"}},\n",
        "    {{\"phrase\": \"piece of cake\", \"verbose_definition\": \"<see #2 above>\", \"usage\": \"<see #3 above>\", \"definition\": \"<see #4 above>\", \"other_definition\": \"<see #5 above>\" }}\n",
        "]}}\n",
        "\n",
        "In your definitions, do not preface your definition with phrases like, \"The literal meaning would be...\" or \"The figurative meaning is...\". Just give the definition, such that it could be used in a downstream task.\n",
        "These are the samples:\n",
        "{examples}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        content = json.loads(response.choices[0].message.content.strip())\n",
        "        print(json.dumps(content, indent=4))\n",
        "        results = {item[\"phrase\"]: item[\"definition\"] for item in content[\"samples\"]}\n",
        "        # for item in content[\"samples\"]:\n",
        "        #     print(f\"Phrase: {item['phrase']}\\nVerbose: {item['verbose_definition']}\\nUsage: {item['usage']}\\nDefinition: {item['definition']}\\nOther Definition: {item['other_definition']}\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating paraphrases: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "yGmwiwOtfXOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to the DataFrame in batches\n",
        "batch_size = 5\n",
        "results = {}\n",
        "for i in range(0, len(df), batch_size):\n",
        "    batch = df.iloc[i:i + batch_size]\n",
        "    compounds = batch['compound'].tolist()\n",
        "    sentences = batch['sentence'].tolist()\n",
        "    batch_results = generate_paraphrases_batched_3(compounds, sentences)\n",
        "    results.update(batch_results)\n",
        "\n",
        "# Map results back to DataFrame\n",
        "df['gpt_prompt_3_resp'] = df['compound'].map(results)"
      ],
      "metadata": {
        "id": "Sz1AW8RSfXOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['compound', 'sentence_type', 'sentence', 'gpt_prompt_1_resp', 'gpt_prompt_2_resp', 'gpt_prompt_3_resp']].to_csv('gpt_prompt_responses.csv', index=False)"
      ],
      "metadata": {
        "id": "vDAl6d5WglU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPT Prompt Experiment 4 (new)"
      ],
      "metadata": {
        "id": "xWGGZrLU4UqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Prompt #4\n",
        "\n",
        "import json\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_PROJECT_KEY'))\n",
        "\n",
        "def generate_paraphrases_batched_2(compound, sentence):\n",
        "    \"\"\"\n",
        "    Generate paraphrases in batches using GPT-4.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are a linguistics expert specializing in idioms. For each of the idioms below, do the following steps aloud (in writing):\n",
        "1. Give a verbose explanation of the idiom, including what connotations it carries or undertones it evokes.\n",
        "2. List three potential definitions, no longer than 20 words each, that capture the essence of the phrase in a general manner.\n",
        "3. Choose the best definition.\n",
        "\n",
        "Example #1:\n",
        "Input:\n",
        "{{\n",
        "  \"compound\": \"cold turkey\",\n",
        "  \"sentence\": \"I've decided to go cold turkey on the cigarettes again\"\n",
        "}}\n",
        "Idiom: \"cold turkey\"\n",
        "Definition: \"Abruptly quitting a habit or addiction, overcoming discomfort or pain, requiring determination.\"\n",
        "\n",
        "Example #2:\n",
        "Idiom: \"piece of cake\"\n",
        "Definition: \"Easy, simple to accomplish, requiring little effort, not a problem.\"\n",
        "\n",
        "Input:\n",
        "{compound}\n",
        "\n",
        "Respond in this format:\n",
        "{{\"idioms\": [\n",
        "    {{\"idiom\": \"cold turkey\", \"verbose_definition\": \"<verbose_definition>\", \"possible_definitions\": \"<three possible definitions>\", \"definition\": \"Abruptly quitting a habit, marked by discomfort and determination.\"}},\n",
        "    {{\"idiom\": \"<another idiom>\", \"verbose_definition\": \"<verbose_definition>\", \"possible_definitions\": \"<three possible definitions>\" , \"definition\": \"<its definition>\" }}\n",
        "]}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        content = json.loads(response.choices[0].message.content.strip())\n",
        "        results = {item[\"idiom\"]: item[\"definition\"] for item in content[\"idioms\"]}\n",
        "        for item in content[\"idioms\"]:\n",
        "            print(f\"Idiom: {item['idiom']}\\nVerbose: {item['verbose_definition']}\\nPossible: {item['possible_definitions']}\\nDefinition: {item['definition']}\\n\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating paraphrases: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "Xj63QQlQ4c8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment and model configurations"
      ],
      "metadata": {
        "id": "-WpYrv_IWIai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiments config"
      ],
      "metadata": {
        "id": "wJ37qEGzTiIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiments = [\n",
        "    {\n",
        "        \"name\": \"Baseline (Sentences)\",\n",
        "        \"text_inputs\": df['sentence']\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"NC-Only\",\n",
        "        \"text_inputs\": df['compound']\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPT Prompt 1\",\n",
        "        \"text_inputs\": df['gpt_prompt_1_resp']\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPT Prompt 2\",\n",
        "        \"text_inputs\": df['gpt_prompt_2_resp']\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPT Prompt 3\",\n",
        "        \"text_inputs\": df['gpt_prompt_3_resp']\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPT Prompt 3 with nc for literal\",\n",
        "        \"text_inputs\": df['gpt_prompt_3_with_nc_for_literal']\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPT Prompt 3 with sentence for literal\",\n",
        "        \"text_inputs\": df['gpt_prompt_3_with_sentence_for_literal']\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "XHmKFDbtTdqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiments.append({\n",
        "        \"name\": \"GPT Prompt 4 with sentence if predicted literal\",\n",
        "        \"text_inputs\": df['text_input_exp_4']\n",
        "    }) # to include prompt 4"
      ],
      "metadata": {
        "id": "THWzmHH3Qp8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Models config"
      ],
      "metadata": {
        "id": "ZtIai-zDl1tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlignProcessor, AlignModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "models = [\n",
        "    {\n",
        "        \"base_model\": \"CLIP\",\n",
        "        \"model_name\": \"ViT-B/32\",\n",
        "        \"model\": clip.load(\"ViT-B/32\", device)[0],\n",
        "        \"preprocess\": clip.load(\"ViT-B/32\", device)[1]\n",
        "    },\n",
        "    {\n",
        "        \"base_model\": \"CLIP\",\n",
        "        \"model_name\": \"ViT-L/14\",\n",
        "        \"model\": clip.load(\"ViT-L/14\", device)[0],\n",
        "        \"preprocess\": clip.load(\"ViT-L/14\", device)[1]\n",
        "    },\n",
        "    {\n",
        "        \"base_model\": \"CLIP\",\n",
        "        \"model_name\": \"RN50x64\",\n",
        "        \"model\": clip.load(\"RN50x64\", device)[0],\n",
        "        \"preprocess\": clip.load(\"RN50x64\", device)[1]\n",
        "    },\n",
        "    {\n",
        "        \"base_model\": \"Align\",\n",
        "        \"model_name\": \"Base\",\n",
        "        \"model\": AlignModel.from_pretrained(\"kakaobrain/align-base\"),\n",
        "        \"preprocess\": AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "pd77vRZCFHOD",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613594b6-ec90-4f55-8421-089cb72ebc2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 96.0MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openclip_model_version = \"ViT-B-32\"\n",
        "model_openclip, _, preprocess_openclip = open_clip.create_model_and_transforms(openclip_model_version, pretrained='laion2b_s34b_b79k')\n",
        "model_openclip.to(device)\n",
        "model_openclip.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
        "\n",
        "models.append({\n",
        "    \"base_model\": \"open_clip\",\n",
        "    \"model_name\": openclip_model_version,\n",
        "    \"model\": model_openclip,\n",
        "    \"preprocess\": preprocess_openclip\n",
        "})"
      ],
      "metadata": {
        "id": "D3Ik9-e_OQlO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "46e263745bfe42cba1641ccd22a583df",
            "009c03518f504c37929deeb8fdc1f45f",
            "6f63427ed5524f2b853db233a9930ba1",
            "c9996590005b4a6e8452ab677d8b8bdd",
            "e8d60c1dc5984c828b91609211e2542e",
            "789460fd175a46dfbaeec46cada512b7",
            "a16b29c632334c38afaa2f0c5300d17a",
            "8f90ee054b17415795c3c85d1984c96d",
            "2fc90f4f752141dfb14c4d4e02c85c1a",
            "322054a5c1c547d197c1213ebed9ffea",
            "56537b9780ab48489916fc4ad79ff42f"
          ]
        },
        "outputId": "e6fbc768-dc53-47f0-d319-c934a56743dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46e263745bfe42cba1641ccd22a583df"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RUN ALL"
      ],
      "metadata": {
        "id": "Ft0xM40xTWTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all experiments on all models\n",
        "for exp in experiments:\n",
        "    print(f\"\\nRunning \\\"{exp['name']}\\\" on {len(models)} models...\")\n",
        "\n",
        "    for idx, conf in enumerate(models):\n",
        "      print(f\"\\nModel [{idx+1}/{len(models)}]: {conf['model_name']} ({conf['base_model']})\")\n",
        "\n",
        "      metrics, preds, confidence = run_experiment(\n",
        "          model=conf['model'],\n",
        "          processor=conf['preprocess'],\n",
        "          df=df,\n",
        "          image_paths=df['image_paths'],\n",
        "          text_inputs=exp['text_inputs'],\n",
        "          model_name=conf['model_name'],\n",
        "          experiment_name=exp['name'],\n",
        "          base_model=conf['base_model']\n",
        "      )\n",
        "      save_predictions(\n",
        "          df=df,\n",
        "          image_paths=df['image_paths'],\n",
        "          predictions=preds,\n",
        "          confidence_scores=confidence,\n",
        "          metrics=metrics,\n",
        "          prefix=f\"{conf['base_model']}_{conf['model_name']}_{exp['name'].lower()}\"\n",
        "      )"
      ],
      "metadata": {
        "id": "0TLSC5-EUJw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5585fa35-e838-4214-f3a7-69acf34d548b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running \"GPT Prompt 4 with sentence if predicted literal\" on 5 models...\n",
            "\n",
            "Model [1/5]: ViT-B/32 (CLIP)\n",
            "Results saved to experiment_results.csv\n",
            "Top-1 Accuracy: 57.14%\n",
            "Average Spearman Correlation: 0.20\n",
            "Average Weighted Accuracy: 0.43\n",
            "Predictions saved to predictions/CLIP_ViT-B32_gpt_prompt_4_with_sentence_if_predicted_literal_preds.csv\n",
            "\n",
            "Model [2/5]: ViT-L/14 (CLIP)\n",
            "Results saved to experiment_results.csv\n",
            "Top-1 Accuracy: 65.71%\n",
            "Average Spearman Correlation: 0.24\n",
            "Average Weighted Accuracy: 0.49\n",
            "Predictions saved to predictions/CLIP_ViT-L14_gpt_prompt_4_with_sentence_if_predicted_literal_preds.csv\n",
            "\n",
            "Model [3/5]: RN50x64 (CLIP)\n",
            "Results saved to experiment_results.csv\n",
            "Top-1 Accuracy: 61.43%\n",
            "Average Spearman Correlation: 0.27\n",
            "Average Weighted Accuracy: 0.43\n",
            "Predictions saved to predictions/CLIP_RN50x64_gpt_prompt_4_with_sentence_if_predicted_literal_preds.csv\n",
            "\n",
            "Model [4/5]: Base (Align)\n",
            "Results saved to experiment_results.csv\n",
            "Top-1 Accuracy: 67.14%\n",
            "Average Spearman Correlation: 0.20\n",
            "Average Weighted Accuracy: 0.49\n",
            "Predictions saved to predictions/Align_Base_gpt_prompt_4_with_sentence_if_predicted_literal_preds.csv\n",
            "\n",
            "Model [5/5]: ViT-B-32 (open_clip)\n",
            "Results saved to experiment_results.csv\n",
            "Top-1 Accuracy: 62.86%\n",
            "Average Spearman Correlation: 0.23\n",
            "Average Weighted Accuracy: 0.44\n",
            "Predictions saved to predictions/open_clip_ViT-B-32_gpt_prompt_4_with_sentence_if_predicted_literal_preds.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df[['compound', 'sentence_type', 'sentence', 'gpt_prompt_1_resp', 'gpt_prompt_2_resp', 'gpt_prompt_3_resp', 'gpt_prompt_3_with_nc_for_literal', 'gpt_prompt_3_with_sentence_for_literal']].to_csv('gpt_prompt_responses_2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "fGfokMwFJ0_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deprecated (run specific standalone experiments)"
      ],
      "metadata": {
        "id": "4tYIywyYXLqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline 1: Sentence only"
      ],
      "metadata": {
        "id": "HHui7nG3OjuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline: Full context sentence\n",
        "exp_name = \"Baseline (Sentences)\"\n",
        "\n",
        "print(f\"\\nRunning \\\"{exp_name}\\\" on {len(models)} models...\")\n",
        "for idx, conf in enumerate(models):\n",
        "  print(f\"\\nModel [{idx+1}/{len(models)}]: {conf['model_name']} ({conf['base_model']})\")\n",
        "\n",
        "  metrics, preds, confidence = run_experiment(\n",
        "      model=conf['model'],\n",
        "      processor=conf['preprocess'],\n",
        "      df=df,\n",
        "      image_paths=df['image_paths'],\n",
        "      text_inputs=df['compound'],\n",
        "      model_name=conf['model_name'],\n",
        "      experiment_name=exp_name,\n",
        "      base_model=conf['base_model']\n",
        "  )\n",
        "  save_predictions(\n",
        "      df=df,\n",
        "      image_paths=df['image_paths'],\n",
        "      predictions=preds,\n",
        "      confidence_scores=confidence,\n",
        "      metrics=metrics,\n",
        "      prefix=f\"{conf['base_model']}_{conf['model_name']}_{exp_name.lower()}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "vVHB9rhTpP5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline 2: NC only"
      ],
      "metadata": {
        "id": "i6n4c9bBe_6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Experiment 2: NC-only\n",
        "exp_name = \"NC-Only\"\n",
        "\n",
        "print(f\"\\nRunning \\\"{exp_name}\\\" on {len(models)} models...\")\n",
        "for idx, conf in enumerate(models):\n",
        "  print(f\"\\nModel [{idx+1}/{len(models)}]: {conf['model_name']} ({conf['base_model']})\")\n",
        "\n",
        "  metrics, preds, confidence = run_experiment(\n",
        "      model=conf['model'],\n",
        "      processor=conf['preprocess'],\n",
        "      df=df,\n",
        "      image_paths=df['image_paths'],\n",
        "      text_inputs=df['compound'],\n",
        "      model_name=conf['model_name'],\n",
        "      experiment_name=exp_name,\n",
        "      base_model=conf['base_model']\n",
        "  )\n",
        "  save_predictions(\n",
        "      df=df,\n",
        "      image_paths=df['image_paths'],\n",
        "      predictions=preds,\n",
        "      confidence_scores=confidence,\n",
        "      metrics=metrics,\n",
        "      prefix=f\"{conf['base_model']}_{conf['model_name']}_{exp_name}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "gcpxdPCyp6P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Experiment 3"
      ],
      "metadata": {
        "id": "6s2KxV54fG5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2: NC-only\n",
        "exp_name = \"GPT Prompt 1\"\n",
        "\n",
        "print(f\"\\nRunning \\\"{exp_name}\\\" on {len(models)} models...\")\n",
        "for idx, conf in enumerate(models):\n",
        "  print(f\"\\nModel [{idx+1}/{len(models)}]: {conf['model_name']} ({conf['base_model']})\")\n",
        "\n",
        "  metrics, preds, confidence = run_experiment(\n",
        "      model=conf['model'],\n",
        "      processor=conf['preprocess'],\n",
        "      df=df,\n",
        "      image_paths=df['image_paths'],\n",
        "      text_inputs=df['compound'],\n",
        "      model_name=conf['model_name'],\n",
        "      experiment_name=exp_name,\n",
        "      base_model=conf['base_model']\n",
        "  )\n",
        "  save_predictions(\n",
        "      df=df,\n",
        "      image_paths=df['image_paths'],\n",
        "      predictions=preds,\n",
        "      confidence_scores=confidence,\n",
        "      metrics=metrics,\n",
        "      prefix=f\"{conf['base_model']}_{conf['model_name']}_{exp_name.lower()}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "hS8i0UiRptum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r results_1751.zip predictions/ experiment_results.csv gpt_prompt_responses_2.csv\n",
        "from google.colab import files\n",
        "files.download('results_1751.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "58cG1UxQfXD1",
        "outputId": "d28d2fee-d75c-4906-d42f-8c89c3d9c456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: predictions/ (stored 0%)\n",
            "  adding: predictions/CLIP_ViT-L14_gpt_prompt_3_with_sentence_for_literal_preds.csv (deflated 64%)\n",
            "  adding: predictions/CLIP_ViT-B32_gpt_prompt_3_preds.csv (deflated 62%)\n",
            "  adding: predictions/CLIP_ViT-B32_gpt_prompt_3_with_nc_for_literal_preds.csv (deflated 62%)\n",
            "  adding: predictions/CLIP_ViT-L14_gpt_prompt_3_with_nc_for_literal_preds.csv (deflated 63%)\n",
            "  adding: predictions/CLIP_ViT-L14_gpt_prompt_3_preds.csv (deflated 63%)\n",
            "  adding: predictions/CLIP_ViT-L14_gpt_prompt_2_preds.csv (deflated 63%)\n",
            "  adding: predictions/Align_Base_gpt_prompt_1_preds.csv (deflated 62%)\n",
            "  adding: predictions/open_clip_ViT-B-32_gpt_prompt_2_preds.csv (deflated 65%)\n",
            "  adding: predictions/Align_Base_gpt_prompt_3_preds.csv (deflated 62%)\n",
            "  adding: predictions/CLIP_ViT-B32_gpt_prompt_3_with_sentence_for_literal_preds.csv (deflated 62%)\n",
            "  adding: predictions/Align_Base_gpt_prompt_3_with_sentence_for_literal_preds.csv (deflated 62%)\n",
            "  adding: predictions/CLIP_RN50x64_gpt_prompt_2_preds.csv (deflated 63%)\n",
            "  adding: predictions/CLIP_RN50x64_gpt_prompt_1_preds.csv (deflated 63%)\n",
            "  adding: predictions/CLIP_RN50x64_gpt_prompt_3_with_sentence_for_literal_preds.csv (deflated 63%)\n",
            "  adding: predictions/CLIP_RN50x64_gpt_prompt_3_preds.csv (deflated 63%)\n",
            "  adding: predictions/Align_Base_gpt_prompt_2_preds.csv (deflated 62%)\n",
            "  adding: predictions/open_clip_ViT-B-32_gpt_prompt_1_preds.csv (deflated 65%)\n",
            "  adding: predictions/CLIP_ViT-B32_gpt_prompt_1_preds.csv (deflated 62%)\n",
            "  adding: predictions/CLIP_RN50x64_gpt_prompt_3_with_nc_for_literal_preds.csv (deflated 63%)\n",
            "  adding: predictions/CLIP_ViT-L14_gpt_prompt_1_preds.csv (deflated 63%)\n",
            "  adding: predictions/open_clip_ViT-B-32_gpt_prompt_3_preds.csv (deflated 64%)\n",
            "  adding: predictions/Align_Base_gpt_prompt_3_with_nc_for_literal_preds.csv (deflated 62%)\n",
            "  adding: predictions/CLIP_ViT-B32_gpt_prompt_2_preds.csv (deflated 62%)\n",
            "  adding: predictions/open_clip_ViT-B-32_gpt_prompt_3_with_nc_for_literal_preds.csv (deflated 65%)\n",
            "  adding: predictions/open_clip_ViT-B-32_gpt_prompt_3_with_sentence_for_literal_preds.csv (deflated 65%)\n",
            "  adding: experiment_results.csv (deflated 79%)\n",
            "  adding: gpt_prompt_responses_2.csv (deflated 69%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e2e08b14-48bb-4ac7-8e26-c9740a31debe\", \"results_1751.zip\", 82881)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}